{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Using MNIST Dataset try to build your Convolutional Neural Network:\n",
        "#A#\n",
        "Do some necessary imports like:\n",
        "Download the MNIST dataset through Keras\n",
        "Import a sequential model\n",
        "Import the convolution and pooling layers\n",
        "Import dense layers, dropout layer, and the flatten layer\n",
        "Import numpy\n",
        "#B##\n",
        "Fit the dataset to a model, i.e train the model for 12 epochs.\n",
        "After training the model, evaluate the loss and accuracy of the model on the test data and print it.\n"
      ],
      "metadata": {
        "id": "u63U-GCI5ELt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWZvfGxt2jAx",
        "outputId": "60a68dd2-783e-4504-a8a4-23bbe673fb04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "300/300 - 52s - 172ms/step - accuracy: 0.8928 - loss: 0.3489\n",
            "Epoch 2/12\n",
            "300/300 - 82s - 273ms/step - accuracy: 0.9693 - loss: 0.1047\n",
            "Epoch 3/12\n",
            "300/300 - 52s - 175ms/step - accuracy: 0.9762 - loss: 0.0775\n",
            "Epoch 4/12\n",
            "300/300 - 52s - 173ms/step - accuracy: 0.9812 - loss: 0.0641\n",
            "Epoch 5/12\n",
            "300/300 - 50s - 168ms/step - accuracy: 0.9836 - loss: 0.0548\n",
            "Epoch 6/12\n",
            "300/300 - 83s - 277ms/step - accuracy: 0.9860 - loss: 0.0462\n",
            "Epoch 7/12\n",
            "300/300 - 83s - 276ms/step - accuracy: 0.9870 - loss: 0.0431\n",
            "Epoch 8/12\n",
            "300/300 - 49s - 164ms/step - accuracy: 0.9883 - loss: 0.0389\n",
            "Epoch 9/12\n",
            "300/300 - 50s - 168ms/step - accuracy: 0.9880 - loss: 0.0371\n",
            "Epoch 10/12\n",
            "300/300 - 81s - 270ms/step - accuracy: 0.9902 - loss: 0.0323\n",
            "Epoch 11/12\n",
            "300/300 - 82s - 272ms/step - accuracy: 0.9909 - loss: 0.0289\n",
            "Epoch 12/12\n",
            "300/300 - 83s - 277ms/step - accuracy: 0.9920 - loss: 0.0270\n",
            "Test Loss: 0.0281\n",
            "Test Accuracy: 0.9912\n"
          ]
        }
      ],
      "source": [
        "# Necessary imports\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape the dataset to be compatible with Conv2D\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# Normalize the images (scale values between 0 and 1)\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoded vectors\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Add convolutional layer with 32 filters, 3x3 kernel, and ReLU activation\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "\n",
        "# Add max-pooling layer with 2x2 pool size\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add another convolutional layer with 64 filters, 3x3 kernel\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "# Add another max-pooling layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten the output of the last pooling layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a dense fully connected layer with 128 neurons and ReLU activation\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Add dropout layer with a dropout rate of 0.5 to avoid overfitting\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the output layer with 10 neurons (for 10 classes) and softmax activation\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model for 12 epochs\n",
        "model.fit(x_train, y_train, epochs=12, batch_size=200, verbose=2)\n",
        "\n",
        "# Evaluate the model on the test data and print the loss and accuracy\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test Loss: {loss:.4f}')\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n"
      ]
    }
  ]
}